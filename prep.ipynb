{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 116 entries, 0 to 115\n",
      "Series name: full_text\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "116 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# read dts.xlsx and dts2.xlsx\n",
    "dts = pd.read_excel('dts.xlsx')\n",
    "dts2 = pd.read_excel('dts2.xlsx')\n",
    "# hanya ambil full_text\n",
    "dts = dts['full_text']\n",
    "dts2 = dts2['full_text']\n",
    "\n",
    "# gabungkan dts dan dts2\n",
    "df = pd.concat([dts, dts2], ignore_index=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Padahal buka aja kalo emang yakin pemerintah u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya perhatikan hampir semua akun akun besar d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biar adil, mahasiswa KKN di Papua\\nTNI ambil S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalian judulnya jurnalis tapi misi sebenarnya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THIS emang TNI itu dikasih belajar HAM gak sih...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text\n",
       "0  Padahal buka aja kalo emang yakin pemerintah u...\n",
       "1  Saya perhatikan hampir semua akun akun besar d...\n",
       "2  Biar adil, mahasiswa KKN di Papua\\nTNI ambil S...\n",
       "3  Kalian judulnya jurnalis tapi misi sebenarnya ...\n",
       "4  THIS emang TNI itu dikasih belajar HAM gak sih..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df to dataframe\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\GLOBAL\n",
      "[nltk_data]     KOMPUTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lakukan praproses dalam bahasa indonesia\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# remove number\n",
    "\n",
    "# define stemming function\n",
    "def stemming(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "def remove_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# remove whitespace\n",
    "def remove_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "nltk.download('stopwords')\n",
    "df['full_text'] = df['full_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    buka aja kalo emang perintah udah bener mah ka...\n",
       "1    perhati akun akun indonesia instagram x kompak...\n",
       "2    biar adil mahasiswa kkn papua tni ambil simak ...\n",
       "3    judul jurnalis misi separatis ya negara yg kas...\n",
       "4    this emang tni kasih ajar ham gak sih angkat t...\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_text'] = df['full_text'].apply(remove_punctuation)\n",
    "df['full_text'] = df['full_text'].apply(remove_number)\n",
    "df['full_text'] = df['full_text'].apply(remove_whitespace)\n",
    "df['full_text'] = df['full_text'].apply(remove_stopwords)\n",
    "df['full_text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GLOBAL KOMPUTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\GLOBAL KOMPUTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "pretrained= \"mdhugol/indonesia-bert-sentiment-classification\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)   \n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_index = {'LABEL_0': 'positive', 'LABEL_1': 'neutral', 'LABEL_2': 'negative'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buka aja kalo emang pemerintah udah bener mah ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perhatikan akun akun indonesia instagram X kom...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biar adil mahasiswa KKN Papua TNI ambil Simak ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>judulnya jurnalis misi separatis Ya negara yg ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THIS emang TNI dikasih belajar HAM gak sih ang...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  sentimen\n",
       "0  buka aja kalo emang pemerintah udah bener mah ...  negative\n",
       "1  perhatikan akun akun indonesia instagram X kom...   neutral\n",
       "2  Biar adil mahasiswa KKN Papua TNI ambil Simak ...  negative\n",
       "3  judulnya jurnalis misi separatis Ya negara yg ...  negative\n",
       "4  THIS emang TNI dikasih belajar HAM gak sih ang...  negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_label(text):\n",
    "    return sentiment_analysis(text)[0]\n",
    "\n",
    "df['sentimen'] = df['full_text'].apply(predict_label)\n",
    "df['sentimen'] = df['sentimen'].apply(lambda x: label_index[x['label']])\n",
    "df[['full_text', 'sentimen']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentimen\n",
       "neutral     55\n",
       "negative    52\n",
       "positive     9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = df[['full_text', 'sentimen']]\n",
    "processed['sentimen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buka aja kalo emang pemerintah udah bener mah ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perhatikan akun akun indonesia instagram X kom...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biar adil mahasiswa KKN Papua TNI ambil Simak ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>judulnya jurnalis misi separatis Ya negara yg ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THIS emang TNI dikasih belajar HAM gak sih ang...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  sentimen\n",
       "0  buka aja kalo emang pemerintah udah bener mah ...  negative\n",
       "1  perhatikan akun akun indonesia instagram X kom...   neutral\n",
       "2  Biar adil mahasiswa KKN Papua TNI ambil Simak ...  negative\n",
       "3  judulnya jurnalis misi separatis Ya negara yg ...  negative\n",
       "4  THIS emang TNI dikasih belajar HAM gak sih ang...  negative"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GLOBAL KOMPUTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Some words would be broken into alphabets and a double hash(##) was added in front of w and p. This technique used by BERT is known as <b>Subword tokenization </b>. The “##” symbol is used to indicate that a token is part of a larger word.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tni',\n",
       " 'bagus',\n",
       " 'ayo',\n",
       " 'aman',\n",
       " '##kan',\n",
       " 'bumi',\n",
       " 'indonesia',\n",
       " 'dari',\n",
       " 'serangan',\n",
       " 'musuh',\n",
       " 'kalau',\n",
       " 'main',\n",
       " 'tentara',\n",
       " 'tentara',\n",
       " '##an',\n",
       " 'yang',\n",
       " 'bener',\n",
       " 'dong',\n",
       " 'jangan',\n",
       " 'asal',\n",
       " 'main']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_1='TNI bagus ayo amankan bumi indonesia dari serangan musuh'\n",
    "test_input_2='Kalau main tentara tentaraan yang bener dong jangan asal main'\n",
    "inputs=[test_input_1,test_input_2]\n",
    "\n",
    "tokenizer.tokenize(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[3, 3136, 4839, 15124, 4022, 1493, 2859, 1718, 1542, 3416, 5084, 4, 0], [3, 2280, 5539, 3682, 3682, 1476, 1497, 13927, 6773, 3132, 2785, 5539, 4]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "output=tokenizer(inputs,padding=True,truncation=True,max_length=128)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] tni bagus ayo amankan bumi indonesia dari serangan musuh [SEP] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "  return tokenizer(examples[\"text\"],padding=True,truncation=True,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [input_ids, token_type_ids, attention_mask]\n",
       "1    [input_ids, token_type_ids, attention_mask]\n",
       "2    [input_ids, token_type_ids, attention_mask]\n",
       "3    [input_ids, token_type_ids, attention_mask]\n",
       "4    [input_ids, token_type_ids, attention_mask]\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply preprocess function to our text data\n",
    "encoded_data = df['full_text'].apply(lambda x: tokenizer(x,padding=True,truncation=True,max_length=128))\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[3, 9012, 6380, 7288, 17673, 1990, 9988, 13927...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>buka aja kalo emang pemerintah udah bener mah ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 10245, 7408, 7408, 1718, 23292, 1517, 2230...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>perhatikan akun akun indonesia instagram X kom...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 9421, 6187, 2928, 13174, 4464, 3136, 8009,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Biar adil mahasiswa KKN Papua TNI ambil Simak ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 21934, 8406, 4746, 14958, 3316, 1806, 3798...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>judulnya jurnalis misi separatis Ya negara yg ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 7185, 17673, 3136, 21463, 3109, 2972, 7552...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>THIS emang TNI dikasih belajar HAM gak sih ang...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [3, 9012, 6380, 7288, 17673, 1990, 9988, 13927...   \n",
       "1  [3, 10245, 7408, 7408, 1718, 23292, 1517, 2230...   \n",
       "2  [3, 9421, 6187, 2928, 13174, 4464, 3136, 8009,...   \n",
       "3  [3, 21934, 8406, 4746, 14958, 3316, 1806, 3798...   \n",
       "4  [3, 7185, 17673, 3136, 21463, 3109, 2972, 7552...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           full_text  sentimen  \n",
       "0  buka aja kalo emang pemerintah udah bener mah ...  negative  \n",
       "1  perhatikan akun akun indonesia instagram X kom...   neutral  \n",
       "2  Biar adil mahasiswa KKN Papua TNI ambil Simak ...  negative  \n",
       "3  judulnya jurnalis misi separatis Ya negara yg ...  negative  \n",
       "4  THIS emang TNI dikasih belajar HAM gak sih ang...  negative  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make encoded data as a df with its full text\n",
    "encoded_data_df = pd.DataFrame(encoded_data.tolist())\n",
    "encoded_data_df['full_text'] = processed['full_text']\n",
    "encoded_data_df['sentimen'] = processed['sentimen']\n",
    "encoded_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116 entries, 0 to 115\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   attention_mask  116 non-null    object\n",
      " 1   input_ids       116 non-null    object\n",
      " 2   token_type_ids  116 non-null    object\n",
      " 3   full_text       116 non-null    object\n",
      " 4   sentimen        116 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.7+ KB\n"
     ]
    }
   ],
   "source": [
    "encoded_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(\n",
    "    max(len(x) for x in encoded_data_df['attention_mask']),\n",
    "    max(len(x) for x in encoded_data_df['input_ids']),\n",
    "    max(len(x) for x in encoded_data_df['token_type_ids'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def pad_array(arr, max_length):\n",
    "    return arr + [0] * (max_length - len(arr))\n",
    "\n",
    "X = []\n",
    "for i in range(len(encoded_data_df)):\n",
    "    attention_mask = pad_array(encoded_data_df['attention_mask'][i], max_length)\n",
    "    input_ids = pad_array(encoded_data_df['input_ids'][i], max_length)\n",
    "    token_type_ids = pad_array(encoded_data_df['token_type_ids'][i], max_length)\n",
    "    \n",
    "    features = np.concatenate([attention_mask, input_ids, token_type_ids])\n",
    "    X.append(features)\n",
    "X = np.array(X)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['sentimen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5833333333333334\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.60      0.60        10\n",
      "     neutral       0.67      0.57      0.62        14\n",
      "    positive       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.58        24\n",
      "   macro avg       0.42      0.39      0.41        24\n",
      "weighted avg       0.64      0.58      0.61        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GLOBAL KOMPUTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\GLOBAL KOMPUTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\GLOBAL KOMPUTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
